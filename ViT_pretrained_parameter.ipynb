{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import timm\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from timm.data import resolve_data_config\n",
    "from timm.data.transforms_factory import create_transform\n",
    "model = timm.create_model('vit_base_patch16_224', pretrained=True)\n",
    "\n",
    "config = resolve_data_config({}, model=model)\n",
    "transform = create_transform(**config)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[ 0.0300,  1.1600, -0.0800, -0.0800, -0.1300,  0.4300,  0.0400,  0.0800],\n",
      "        [ 0.1200, -0.9800, -0.1800, -0.0000,  0.0900,  0.1400, -0.0200,  0.0600],\n",
      "        [ 0.1000, -0.1400, -0.0300,  0.2200,  0.0600,  0.2700,  0.1100,  1.1400],\n",
      "        [ 0.0900,  1.3500, -0.1500,  0.0600, -0.0700,  0.2500, -0.0900,  0.2300],\n",
      "        [-0.1900, -0.0500, -0.0500, -0.0900, -0.1700,  0.3300, -0.0600,  0.3900],\n",
      "        [ 0.2500,  0.3900,  0.0200,  0.1100, -0.0500,  0.4200,  0.1300,  0.3300],\n",
      "        [ 0.0200,  0.7100, -0.0200,  0.0400,  0.0700,  0.1300, -0.0900, -0.3500],\n",
      "        [-0.0100, -0.0600, -0.0500,  0.1400, -0.1700,  0.1300, -0.1300, -0.0400]],\n",
      "       grad_fn=<DivBackward0>)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "tensor([[ 0.0300,  1.1600, -0.0800, -0.0800, -0.1300,  0.4300,  0.0400,  0.0800],\n",
       "        [ 0.1200, -0.9800, -0.1800, -0.0000,  0.0900,  0.1400, -0.0200,  0.0600],\n",
       "        [ 0.1000, -0.1400, -0.0300,  0.2200,  0.0600,  0.2700,  0.1100,  1.1400],\n",
       "        [ 0.0900,  1.3500, -0.1500,  0.0600, -0.0700,  0.2500, -0.0900,  0.2300],\n",
       "        [-0.1900, -0.0500, -0.0500, -0.0900, -0.1700,  0.3300, -0.0600,  0.3900],\n",
       "        [ 0.2500,  0.3900,  0.0200,  0.1100, -0.0500,  0.4200,  0.1300,  0.3300],\n",
       "        [ 0.0200,  0.7100, -0.0200,  0.0400,  0.0700,  0.1300, -0.0900, -0.3500],\n",
       "        [-0.0100, -0.0600, -0.0500,  0.1400, -0.1700,  0.1300, -0.1300, -0.0400]],\n",
       "       grad_fn=<DivBackward0>)"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# dc = model.named_children()\n",
    "# dc = list(dc)\n",
    "# x = torch.arange(4*3*224*224, dtype=torch.float32).reshape(4, 3, 224, 224)\n",
    "# x.shape\n",
    "torch.manual_seed(10)\n",
    "x = torch.randn(4, 3, 224, 224)\n",
    "print(torch.round(dict(dict(model.named_children())['patch_embed'].named_children())['proj'](x).flatten(2).transpose(1, 2)[1,188:,:8]*100)/100)\n",
    "W = dict(dict(model.named_children())['patch_embed'].named_children())['proj'].weight.flatten(1) #3x16x16을 768로\n",
    "b = dict(dict(model.named_children())['patch_embed'].named_children())['proj'].bias\n",
    "# print(x.reshape(4, 3, 14, 16, 14, 16).permute(0,2,4,1,3,5).reshape(4, -1, 768).shape)\n",
    "\n",
    "torch.round(torch.add(torch.matmul(x.reshape(4, 3, 14, 16, 14, 16).permute(0,2,4,1,3,5).reshape(4, -1, 768), W.T), b)[1,188:,:8]*100)/100\n",
    "# print(W.shape)\n",
    "# dict(model.named_children())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## WEIGHT binaryfile 생성\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### EMBEDDING"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(768, 768) (768,)\n"
     ]
    }
   ],
   "source": [
    "W = dict(dict(model.named_children())['patch_embed'].named_children())['proj'].weight.flatten(1).clone().detach().numpy().T #3x16x16을 768로\n",
    "b = dict(dict(model.named_children())['patch_embed'].named_children())['proj'].bias.clone().detach().numpy()\n",
    "print(W.shape, b.shape)\n",
    "# W.tofile('./pre_weights/embedding.bin')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### BLOCK"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "for layer in range(12):\n",
    "    block = dict(dict(dict(model.named_children())['blocks'].named_children())[str(layer)].named_children())\n",
    "    ls = [block['norm1'],list(block['attn'].children())[0], list(block['attn'].children())[4],block['norm2'],list(block['mlp'].children())[0], list(block['mlp'].children())[4]]\n",
    "\n",
    "    weight_ls = list()\n",
    "    for i in range(len(ls)):\n",
    "        weight_ls.append(ls[i].weight.clone().detach().numpy().T)\n",
    "        weight_ls.append(ls[i].bias.clone().detach().numpy())\n",
    "\n",
    "    blk_weight = np.array(weight_ls[0].flatten())\n",
    "\n",
    "    for i in range(1, len(weight_ls)):\n",
    "        blk_weight = np.concatenate((blk_weight, weight_ls[i].flatten()))\n",
    "    # blk_weight.tofile('./pre_weights/'+str(layer)+'_newblock'+'.bin')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### QKV WEIGHT BINFILE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "layer = 0\n",
    "dd = dict(list(list(model.named_children())[4][1].children())[layer].named_children())\n",
    "blk = dict(dd['attn'].named_children())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## dummy input check"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.manual_seed(10)\n",
    "dummy_input = torch.randn((4, 196, 768))\n",
    "qkv_w = dict(dd['attn'].named_children())['qkv'].weight.clone().detach().T"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "# dummy_input.numpy().tofile('./pre_weights/dummy_input_4_196_768.bin')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([4, 196, 768])"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dummy_input.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([768, 2304])\n",
      "torch.Size([2304])\n",
      "torch.Size([768, 768])\n",
      "torch.Size([768])\n",
      "torch.Size([768, 3072])\n",
      "torch.Size([3072])\n",
      "torch.Size([3072, 768])\n",
      "torch.Size([768])\n"
     ]
    }
   ],
   "source": [
    "blocks = list()\n",
    "for layer in range(12):\n",
    "    block = dict(dict(dict(model.named_children())['blocks'].named_children())[str(layer)].named_children())\n",
    "    ls = [list(block['attn'].children())[0], list(block['attn'].children())[4],list(block['mlp'].children())[0], list(block['mlp'].children())[4]]\n",
    "\n",
    "    weight_ls = list()\n",
    "    for i in range(len(ls)):\n",
    "        weight_ls.append(ls[i].weight.clone().detach().T)\n",
    "        weight_ls.append(ls[i].bias.clone().detach())\n",
    "    blocks.append(weight_ls)\n",
    "\n",
    "    \n",
    "\n",
    "for i in blocks[0]:\n",
    "    print(i.shape)\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# FLASH ATTENTION VALUE CHECK"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[-0.2055, -0.1736,  0.4550,  0.2223, -0.0058, -0.1951,  0.1081, -0.0950],\n",
       "        [-0.2403, -0.1827,  0.3924,  0.2354, -0.0299, -0.1278,  0.1057, -0.0935],\n",
       "        [-0.2598, -0.1673,  0.4965,  0.2337, -0.0930, -0.1645,  0.0652, -0.0784],\n",
       "        [-0.2632, -0.1369,  0.4431,  0.2356,  0.0452, -0.1379,  0.0822, -0.0350],\n",
       "        [-0.2474, -0.1771,  0.3949,  0.1860,  0.0159, -0.1896,  0.0057, -0.0729],\n",
       "        [-0.2212, -0.1546,  0.4233,  0.1712, -0.0062, -0.1302,  0.0796, -0.0667],\n",
       "        [-0.2667, -0.1787,  0.4048,  0.2312,  0.0124, -0.1626,  0.1034, -0.1029],\n",
       "        [-0.2447, -0.1572,  0.4262,  0.2408,  0.0147, -0.1515,  0.0910, -0.0736]],\n",
       "       grad_fn=<SliceBackward0>)"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "block = dict(dict(dict(model.named_children())['blocks'].named_children())[str(layer)].named_children())\n",
    "# Fetch correct qkv weights and biases from the block\n",
    "qkv_layer = dict(block['attn'].named_children())['qkv']\n",
    "qkv_weights = qkv_layer.weight  # Shape: (out_features, in_features)\n",
    "qkv_bias = qkv_layer.bias\n",
    "\n",
    "# Transpose qkv_weights to shape (in_features, out_features) for matrix multiplication\n",
    "qkv_weights = qkv_weights.T  # Now shape: (in_features, out_features)\n",
    "\n",
    "# Apply the QKV linear projection with the correct transposed weights and biases\n",
    "dQKV = torch.add(torch.matmul(dummy_input, qkv_weights), qkv_bias)\n",
    "\n",
    "# Reshape and permute to get the correct multi-head attention format\n",
    "dQKV = dQKV.view(4, 196, 12, 3, 64)  # (batch, seq_len, num_heads, qkv, head_dim)\n",
    "dQKV = torch.einsum('b p h k d -> b h k p d', dQKV)  # Permute dimensions\n",
    "\n",
    "# Calculate attention manually for each head\n",
    "Os = list()\n",
    "for i in range(12):\n",
    "    Q = dQKV[0][i][0]  # Query for head i\n",
    "    K = dQKV[0][i][1]  # Key for head i\n",
    "    V = dQKV[0][i][2]  # Value for head i\n",
    "    \n",
    "    # Apply scaled dot-product attention with numerical stability\n",
    "    attn_weights = torch.matmul(Q, K.T) / (Q.size(-1) ** 0.5)  # Scale by sqrt(d_k)\n",
    "    attn_weights = attn_weights - torch.max(attn_weights, dim=-1, keepdim=True)[0]  # Numerical stability\n",
    "    attn_weights = torch.nn.functional.softmax(attn_weights, dim=-1)\n",
    "    \n",
    "    O = torch.matmul(attn_weights, V)  # Multiply attention weights by V\n",
    "    Os.append(O)\n",
    "\n",
    "# Concatenate outputs from all heads and apply the projection layer\n",
    "concat_Os = torch.cat(Os, dim=1)  # Concatenate along the feature dimension (head outputs)\n",
    "proj_output = dict(block['attn'].named_children())['proj'](concat_Os)[:8, :8]  # Apply projection layer\n",
    "proj_output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(-1.1279)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "torch.Size([3, 4, 12, 196, 64])"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "blk1 = blocks[0]\n",
    "torch.manual_seed(10)\n",
    "blk1[0] = blk1[0].to(torch.float32)\n",
    "blk1[1] = blk1[1].to(torch.float32)\n",
    "\n",
    "dummy_input = torch.randn((4, 196, 768), dtype=torch.float32)\n",
    "print(torch.matmul(dummy_input,blk1[0])[0][0][1])\n",
    "dQKV = torch.add(torch.matmul(dummy_input,blk1[0]), blk1[1])\n",
    "dQKV = dQKV.view(4, 196, 3, 12, 64)\n",
    "dQKV = torch.einsum('b p k h d -> k b h p d',dQKV)\n",
    "dQKV.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "Qs, Ks, Vs = dQKV.unbind(0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "4 : batch\n",
    "12 : Multi head\n",
    "3 : QKV"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "Q = Qs[0][0]# batch head\n",
    "K = Ks[0][0]\n",
    "V = Vs[0][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([196, 64])"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# torch.matmul(torch.nn.functional.softmax(torch.matmul(Q, K.T), dim=1),V)[188:196,8:16]#일반 ATTN\n",
    "torch.matmul(torch.nn.functional.softmax(torch.matmul(Q, K.T), dim=1),V).shape#일반 ATTN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[ 0.0201,  0.1241,  0.2855, -0.2697, -0.2250,  0.6140, -0.0903, -0.4683],\n",
       "        [ 0.5427, -0.0843,  0.0165, -0.2499, -0.4020,  0.5875, -0.0717, -0.6233],\n",
       "        [ 0.0990, -0.1040,  0.1056, -0.0610, -0.2217,  0.2057, -0.0635, -0.3588],\n",
       "        [ 0.3024, -0.0653, -0.0542, -0.2307, -0.2722,  0.1737, -0.0037, -0.5988],\n",
       "        [ 0.0938,  0.0728,  0.3319, -0.3459, -0.2872,  0.4283,  0.2563, -0.3149],\n",
       "        [ 0.1920, -0.0805,  0.0075, -0.2601, -0.2883,  0.2606, -0.1216, -0.2211],\n",
       "        [ 0.2577, -0.0885,  0.0766, -0.2594, -0.2453,  0.4059,  0.0835, -0.2692],\n",
       "        [ 0.3502, -0.3796,  0.2173, -0.5175, -0.1211,  0.9736,  0.1606, -0.3987]],\n",
       "       grad_fn=<SliceBackward0>)"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Os = list()\n",
    "for i in range(12):\n",
    "    Q = Qs[0][i]# batch head\n",
    "    K = Ks[0][i]\n",
    "    V = Vs[0][i]\n",
    "    O = torch.matmul(torch.nn.functional.softmax(torch.matmul(Q, K.T)/8, dim=1),V)\n",
    "    Os.append(O)\n",
    "torch.concat(Os, dim=1)[:8,:8]\n",
    "dict(block['attn'].named_children())['proj'](torch.concat(Os, dim=1))[:8,:8]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[ 0.1160,  0.0130,  0.1180,  0.0090, -0.0920, -0.2190,  0.1030,  0.2480],\n",
      "        [ 0.2220, -0.0080, -0.1670,  0.0520, -0.0000, -0.0100, -0.0760,  0.3100],\n",
      "        [ 0.1020, -0.0650,  0.1430,  0.0320, -0.0640, -0.2800,  0.0630,  0.3230],\n",
      "        [ 0.0510,  0.0880, -0.0770,  0.2210,  0.0650, -0.2940,  0.0600,  0.1930],\n",
      "        [ 0.0970, -0.0080,  0.1480, -0.0320, -0.0160, -0.2480,  0.1040,  0.0700],\n",
      "        [ 0.1160, -0.1060,  0.1930,  0.0130,  0.0290, -0.1590,  0.1320,  0.1870],\n",
      "        [ 0.0990, -0.0990,  0.0100, -0.0660, -0.0790, -0.1010,  0.1420, -0.0100],\n",
      "        [ 0.1710, -0.0330,  0.1460, -0.1300, -0.1790, -0.2540,  0.1940,  0.0700]],\n",
      "       grad_fn=<DivBackward0>)\n"
     ]
    }
   ],
   "source": [
    "layer = 0\n",
    "block = dict(dict(dict(model.named_children())['blocks'].named_children())[str(layer)].named_children())\n",
    "# list(block['attn'].children())[0]\n",
    "ls = [list(block['attn'].children())[0], list(block['attn'].children())[4],list(block['mlp'].children())[0], list(block['mlp'].children())[4]]\n",
    "ls[0]\n",
    "block['attn']\n",
    "print(torch.round((block['attn'](dummy_input))[2,188:,760:]*1000)/1000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'norm1': LayerNorm((768,), eps=1e-06, elementwise_affine=False),\n",
       " 'attn': Attention(\n",
       "   (qkv): Linear(in_features=768, out_features=2304, bias=True)\n",
       "   (q_norm): Identity()\n",
       "   (k_norm): Identity()\n",
       "   (attn_drop): Dropout(p=0.0, inplace=False)\n",
       "   (proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "   (proj_drop): Dropout(p=0.0, inplace=False)\n",
       " ),\n",
       " 'ls1': Identity(),\n",
       " 'drop_path1': Identity(),\n",
       " 'norm2': LayerNorm((768,), eps=1e-06, elementwise_affine=True),\n",
       " 'mlp': Mlp(\n",
       "   (fc1): Linear(in_features=768, out_features=3072, bias=True)\n",
       "   (act): GELU(approximate='none')\n",
       "   (drop1): Dropout(p=0.0, inplace=False)\n",
       "   (norm): Identity()\n",
       "   (fc2): Linear(in_features=3072, out_features=768, bias=True)\n",
       "   (drop2): Dropout(p=0.0, inplace=False)\n",
       " ),\n",
       " 'ls2': Identity(),\n",
       " 'drop_path2': Identity()}"
      ]
     },
     "execution_count": 57,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "block"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[[-0.7965, -0.5257, -0.8062,  ...,  1.1163, -0.0204, -0.5232],\n",
       "         [ 1.2909,  1.7094,  0.3494,  ...,  0.0838, -1.1657,  0.4946],\n",
       "         [ 0.8116,  0.0457, -1.2625,  ..., -1.4313,  0.3527, -0.9826],\n",
       "         ...,\n",
       "         [-0.1239,  0.6573,  0.1965,  ...,  0.9982, -0.7789,  0.9844],\n",
       "         [ 0.1535, -0.5457,  1.2583,  ...,  1.5386, -0.3528, -0.9368],\n",
       "         [-1.1326,  0.0064,  0.0775,  ..., -0.5256,  0.4120,  0.9336]],\n",
       "\n",
       "        [[ 0.0936,  0.2473,  1.0623,  ..., -0.1770,  0.4844,  1.0437],\n",
       "         [-0.1847,  0.9648, -1.1100,  ...,  1.4537,  0.8065, -0.7966],\n",
       "         [ 0.4875, -0.4403, -0.1437,  ..., -0.6625,  0.9225,  0.9251],\n",
       "         ...,\n",
       "         [ 0.7302, -0.2824,  1.0787,  ...,  0.2373,  0.4794, -1.6490],\n",
       "         [-1.1229,  0.4640, -0.9841,  ...,  1.9941,  0.4529,  1.7247],\n",
       "         [ 0.1536,  0.2029,  0.5519,  ..., -0.6439, -0.6427, -0.5917]],\n",
       "\n",
       "        [[-0.1603, -0.8803,  0.7896,  ..., -1.5139, -0.3209, -0.3525],\n",
       "         [ 0.3014,  0.9013, -0.8356,  ..., -0.8553,  0.7717,  0.8582],\n",
       "         [-0.8185,  0.6779,  1.0356,  ..., -0.9022,  0.9753, -0.8933],\n",
       "         ...,\n",
       "         [ 1.2896,  0.0056,  0.1291,  ..., -0.6712,  1.1102,  1.2918],\n",
       "         [ 0.5790,  2.8034,  1.9539,  ..., -1.0166,  1.1659,  2.9234],\n",
       "         [-0.3910, -0.5879,  0.7980,  ..., -0.9337,  0.3491,  1.2611]],\n",
       "\n",
       "        [[-0.6286,  0.2099,  2.0257,  ..., -0.6309, -1.3870, -1.3539],\n",
       "         [ 1.0148, -0.9909,  0.2651,  ..., -0.9406, -0.1319, -0.5558],\n",
       "         [ 1.1023,  0.9219,  1.0347,  ..., -0.7030,  0.5098,  0.5908],\n",
       "         ...,\n",
       "         [ 1.4299, -0.4787,  0.1326,  ..., -0.2613,  1.7626, -2.1986],\n",
       "         [ 0.5457,  0.9259,  1.1874,  ...,  1.8132, -1.0527, -0.4033],\n",
       "         [ 2.0246, -1.2837, -0.3197,  ...,  0.1387,  0.7217, -0.8300]]])"
      ]
     },
     "execution_count": 62,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# torch.round((nn.LayerNorm((768,),elementwise_affine=False)(dummy_input))[0, :8,:8]*10000)/10000\n",
    "# print((torch.round((block['norm1'](dummy_input))*100)/100)[0,:8,:8])\n",
    "# (torch.round((block['norm1'](dummy_input))*100)/100)[0,:8,:8]\n",
    "nn.LayerNorm((768,), 1e-06, elementwise_affine=False)(dummy_input)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[[-1.7298, -1.7253, -1.7208,  ...,  1.7208,  1.7253,  1.7298],\n",
       "         [-1.7298, -1.7253, -1.7208,  ...,  1.7208,  1.7253,  1.7298],\n",
       "         [-1.7298, -1.7253, -1.7208,  ...,  1.7208,  1.7253,  1.7298],\n",
       "         ...,\n",
       "         [-1.7298, -1.7253, -1.7208,  ...,  1.7208,  1.7253,  1.7298],\n",
       "         [-1.7298, -1.7253, -1.7208,  ...,  1.7208,  1.7253,  1.7298],\n",
       "         [-1.7298, -1.7253, -1.7208,  ...,  1.7208,  1.7253,  1.7298]],\n",
       "\n",
       "        [[-1.7298, -1.7253, -1.7208,  ...,  1.7208,  1.7253,  1.7298],\n",
       "         [-1.7298, -1.7253, -1.7208,  ...,  1.7208,  1.7253,  1.7298],\n",
       "         [-1.7298, -1.7253, -1.7208,  ...,  1.7207,  1.7253,  1.7298],\n",
       "         ...,\n",
       "         [-1.7297, -1.7252, -1.7207,  ...,  1.7208,  1.7253,  1.7299],\n",
       "         [-1.7297, -1.7252, -1.7207,  ...,  1.7208,  1.7253,  1.7298],\n",
       "         [-1.7298, -1.7252, -1.7207,  ...,  1.7208,  1.7253,  1.7298]],\n",
       "\n",
       "        [[-1.7298, -1.7253, -1.7207,  ...,  1.7208,  1.7253,  1.7298],\n",
       "         [-1.7298, -1.7253, -1.7207,  ...,  1.7208,  1.7253,  1.7298],\n",
       "         [-1.7298, -1.7253, -1.7208,  ...,  1.7208,  1.7253,  1.7298],\n",
       "         ...,\n",
       "         [-1.7298, -1.7252, -1.7207,  ...,  1.7208,  1.7253,  1.7298],\n",
       "         [-1.7298, -1.7252, -1.7207,  ...,  1.7208,  1.7253,  1.7298],\n",
       "         [-1.7298, -1.7253, -1.7207,  ...,  1.7208,  1.7253,  1.7298]],\n",
       "\n",
       "        [[-1.7298, -1.7253, -1.7208,  ...,  1.7208,  1.7253,  1.7298],\n",
       "         [-1.7298, -1.7253, -1.7208,  ...,  1.7208,  1.7253,  1.7298],\n",
       "         [-1.7298, -1.7253, -1.7208,  ...,  1.7208,  1.7253,  1.7298],\n",
       "         ...,\n",
       "         [-1.7298, -1.7253, -1.7207,  ...,  1.7208,  1.7253,  1.7298],\n",
       "         [-1.7298, -1.7253, -1.7208,  ...,  1.7208,  1.7253,  1.7298],\n",
       "         [-1.7298, -1.7253, -1.7208,  ...,  1.7208,  1.7253,  1.7298]]])"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "nn.LayerNorm((768,),eps=1e-06, elementwise_affine=False)(torch.arange(4*196*768, dtype=torch.float32).reshape(4, 196, 768))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([ 0.,  1.,  2.,  3.,  4.,  5.,  6.,  7.,  8.,  9., 10., 11., 12., 13.,\n",
       "        14., 15., 16., 17., 18., 19., 20., 21., 22., 23., 24., 25., 26., 27.,\n",
       "        28., 29.])"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.arange(30, dtype=torch.float32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "kkh38",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
